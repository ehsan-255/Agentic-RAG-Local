# Setting Up PostgreSQL + pgvector for Agentic RAG Systems

## Phase 1: Environment Setup

### 1. Prerequisites Installation
1. **PostgreSQL Database Server**
   - Download PostgreSQL installer (version 14+ recommended) from postgresql.org
   - Run the installer with administrative privileges
   - Select components: PostgreSQL Server, pgAdmin (GUI tool), Command Line Tools
   - Choose a data directory with sufficient storage space
   - Set a secure master password for the PostgreSQL superuser

2. **pgvector Extension Installation**
   - Option A (Package Manager):
     - On Ubuntu/Debian: `apt-get install postgresql-14-pgvector`
     - On macOS with Homebrew: `brew install pgvector`
   - Option B (Manual compilation):
     - Install PostgreSQL development headers
     - Clone pgvector repository: `git clone https://github.com/pgvector/pgvector.git`
     - Compile and install the extension
     - Run `make && make install` in the pgvector directory

3. **Network Configuration**
   - Configure PostgreSQL to listen on appropriate network interfaces
   - Edit `postgresql.conf` to set `listen_addresses` parameter
   - Modify `pg_hba.conf` to define access control rules for the application

### 2. Database Configuration
1. **Create Application Database**
   - Connect to PostgreSQL using psql or pgAdmin
   - Create a dedicated database: `CREATE DATABASE agentic_rag;`
   - Create application-specific user with appropriate privileges

2. **Enable pgvector Extension**
   - Connect to the new database
   - Run: `CREATE EXTENSION IF NOT EXISTS vector;`
   - Verify installation with: `SELECT * FROM pg_extension WHERE extname = 'vector';`

3. **Configure Vector Search Parameters**
   - Set appropriate memory parameters for vector operations
   - Configure `maintenance_work_mem` parameter (recommend 1GB+ for large vector datasets)
   - Adjust `max_parallel_workers` for improved search performance

## Phase 2: Schema Creation

### 1. Database Schema Setup
1. **Create Tables with Vector Support**
   - Create `documentation_sources` table for tracking document sources
   - Create `site_pages` table with vector column:
     ```sql
     embedding vector(1536)
     ```
   - Ensure proper data types for all columns, especially JSON/JSONB for metadata

2. **Create Vector Indexes**
   - Implement HNSW index for fast approximate nearest neighbor search:
     ```sql
     CREATE INDEX ON site_pages USING hnsw (embedding vector_cosine_ops);
     ```
   - Consider IVF (Inverted File) index for larger datasets if exact search is less critical

### 2. Function Implementation
1. **Create Database Functions**
   - Create the `match_site_pages` function for vector similarity search
   - Implement `increment_pages_count` and `increment_chunks_count` functions
   - Add hybrid search function for combining lexical and semantic search

2. **Setup Enhanced Functions**
   - Create query transformation function to expand and improve queries
   - Implement hybrid search function that combines BM25 text search with vector similarity
   - Develop functions for enhanced metadata filtering and aggregation

## Phase 3: Application Code Integration

### 1. Connection Management
1. **Database Connection Configuration**
   - Replace Supabase client with direct PostgreSQL connection
   - Implement connection pooling (using libraries like `psycopg_pool` for Python)
   - Set appropriate connection timeouts and retry logic

2. **Authentication Implementation**
   - Implement direct authentication mechanism
   - Set up user management directly in PostgreSQL
   - Configure role-based access controls if needed

### 2. Query Implementation
1. **Vector Search Queries**
   - Implement direct SQL queries for vector search
   - Create parameterized queries to prevent SQL injection
   - Set up vector similarity search logic with appropriate operators (`<=>` for cosine similarity)

2. **Enhanced Query Features**
   - Add query transformation layer that enhances queries before vector search
   - Integrate hybrid search with weighting between keyword and semantic matches
   - Expand metadata filtering capabilities in queries

## Phase 4: Enhancing RAG Features

### 1. Metadata Enhancement
1. **Schema Extensions**
   - Add additional metadata fields to capture more context
   - Create specialized indexes for frequently queried metadata attributes
   - Implement JSON path operators for complex metadata queries

2. **Metadata Extraction Pipeline**
   - Enhance the document processing pipeline to extract richer metadata
   - Implement automatic categorization and tagging
   - Add temporal information to track document freshness

3. **Enhanced Chunk Metadata**
   - Document hierarchy information:
     ```json
     {
       "metadata": {
         "source": "company_docs",
         "source_id": "company_docs_123456",
         "document_type": "api_reference",
         "section": "Authentication",
         "subsection": "OAuth2 Flow",
         "parent_id": "auth_section_123",
         "depth_level": 2
       }
     }
     ```
   - Content classification tags:
     ```json
     {
       "metadata": {
         "content_type": "code_example",
         "programming_language": "python",
         "topics": ["authentication", "security", "oauth"],
         "difficulty_level": "intermediate"
       }
     }
     ```
   - Temporal and versioning data:
     ```json
     {
       "metadata": {
         "created_at": "2023-05-15T10:30:00Z",
         "last_updated": "2024-11-10T14:22:31Z",
         "version": "v2.3.5",
         "is_deprecated": false,
         "expires_at": null
       }
     }
     ```
   - Usage and relevance signals:
     ```json
     {
       "metadata": {
         "view_count": 1250,
         "helpful_votes": 42,
         "query_match_frequency": 0.85,
         "priority_score": 0.92
       }
     }
     ```

### 2. Query Transformation Implementation
1. **Query Understanding Layer**
   - Implement query intent classification
   - Add query expansion to include synonyms and related terms
   - Create specialized handling for different query types (factual, procedural, etc.)

2. **Query Optimization**
   - Implement automatic query refinement based on initial results
   - Add query decomposition for complex questions
   - Develop caching strategy for transformed queries

3. **Query Transformation Function**
   ```python
   async def transform_query(user_query: str, api_semaphore: asyncio.Semaphore) -> Dict[str, Any]:
       """Transform a user query to improve retrieval effectiveness."""
       async with api_semaphore:
           response = await openai_client.chat.completions.create(
               model="gpt-4o-mini",
               messages=[
                   {"role": "system", "content": "You are a query transformation specialist. Your job is to analyze and transform queries to improve document retrieval."},
                   {"role": "user", "content": f"Original query: {user_query}\n\nTransform this query for better retrieval."}
               ],
               functions=[
                   {
                       "name": "transform_query",
                       "description": "Generate transformed queries",
                       "parameters": {
                           "type": "object",
                           "properties": {
                               "semantic_query": {"type": "string", "description": "Rephrased query optimized for semantic search"},
                               "keyword_query": {"type": "string", "description": "Query optimized for keyword search"},
                               "expanded_queries": {"type": "array", "items": {"type": "string"}, "description": "Additional queries that explore different aspects"}
                           }
                       }
                   }
               ],
               function_call={"name": "transform_query"}
           )
           
           transformed_queries = json.loads(response.choices[0].message.function_call.arguments)
           return transformed_queries
   ```

### 3. Hybrid Search Setup
1. **Text Search Configuration**
   - Configure PostgreSQL's full-text search with appropriate dictionaries
   - Create text search indexes on content fields
   - Set up text search configuration for specific language needs

2. **Hybrid Search Function**
   - Create a function that combines vector similarity and text search scores
   - Implement adjustable weighting between the two search methods
   - Add relevance feedback mechanisms to tune search results

3. **Result Ranking Enhancement**
   - Implement custom ranking algorithms that consider multiple factors
   - Add recency bias for newer documents
   - Implement popularity or importance weighting if applicable

4. **Hybrid Search Implementation**
   ```sql
   CREATE OR REPLACE FUNCTION hybrid_search(
     query_text TEXT,
     query_embedding VECTOR(1536),
     match_count INT DEFAULT 10,
     ts_weight FLOAT DEFAULT 0.7,
     vector_weight FLOAT DEFAULT 0.3
   ) RETURNS TABLE (
     id BIGINT,
     url TEXT,
     title TEXT,
     content TEXT,
     metadata JSONB,
     similarity FLOAT
   ) LANGUAGE plpgsql AS $$
   BEGIN
     RETURN QUERY
     WITH 
     ts_results AS (
       SELECT
         id,
         url,
         title,
         content,
         metadata,
         ts_rank(to_tsvector('english', content), plainto_tsquery('english', query_text)) AS ts_score
       FROM site_pages
       WHERE to_tsvector('english', content) @@ plainto_tsquery('english', query_text)
       ORDER BY ts_score DESC
       LIMIT 20
     ),
     vector_results AS (
       SELECT
         id,
         url,
         title,
         content,
         metadata,
         1 - (embedding <=> query_embedding) AS vector_score
       FROM site_pages
       ORDER BY embedding <=> query_embedding
       LIMIT 20
     ),
     combined_results AS (
       SELECT
         COALESCE(ts.id, vr.id) AS id,
         COALESCE(ts.url, vr.url) AS url,
         COALESCE(ts.title, vr.title) AS title,
         COALESCE(ts.content, vr.content) AS content,
         COALESCE(ts.metadata, vr.metadata) AS metadata,
         (COALESCE(ts.ts_score, 0) * ts_weight + COALESCE(vr.vector_score, 0) * vector_weight) AS combined_score
       FROM ts_results ts
       FULL OUTER JOIN vector_results vr ON ts.id = vr.id
     )
     SELECT * FROM combined_results
     ORDER BY combined_score DESC
     LIMIT match_count;
   END;
   $$;
   ```

### 4. Chain-of-Thought Prompting
**System Prompt:**

You are an expert documentation analyst with access to multiple documentation sources through a <<vector database>>. Your responses MUST follow this structured process and formatting:

**PROCESS:**
1. **Question Analysis**
   - Identify key concepts, entities, and required information type (factual, procedural, conceptual, or example-driven)
   - Determine if version-specific details or cross-referenced documentation are needed

2. **Information Retrieval**
   - First, retrieve general context about the topic using <<vector search>> to establish foundational understanding
   - Then, retrieve specific <<documentation chunks>> addressing technical details, examples, or implementation steps

3. **Sufficiency Check**
   - Analyze whether retrieved information directly answers the question
   - If gaps exist, reformulate searches iteratively to target missing components (e.g., narrower keywords, section-specific queries)

4. **Reasoning & Synthesis**
   - Step-by-step, explain how the retrieved documentation addresses the query. Explicitly state:
     - How general context defines scope
     - How specific details resolve subcomponents of the question
     - Any contradictions between sources and why one interpretation is prioritized
   - If documentation is incomplete, state *exactly* what is missing

**RESPONSE FORMAT:**
1. **Direct Answer**
   - Begin with a clear, concise response to the main question

2. **Supporting Information**
   - Then, provide supporting details, examples, or contextual information from the documentation
   - For specific instances, examples, or code, include exact code/examples from the documentation

3. **Structured Output**
   - Structure complex answers with logical flow, clear formatting, and headings
   - Cite documentation sections by **title**, **URL**, and **exact text excerpts**
   - For version-specific answers, specify the version used

**RULES:**
1. **NEVER** use external knowledge. If documentation is insufficient, state: "I cannot find specific information about this."
2. **ALWAYS** prioritize accuracy:
   - Acknowledge contradictions and explain which source is likely current (e.g., newer timestamp, official errata)
   - Reject hypotheticals or assumptions not explicitly stated in documentation
3. **DO NOT** provide code examples unless they exist verbatim in the documentation
4. For multi-part questions, address each component systematically using headings

If uncertain, respond with: "The documentation does not provide enough details to answer this conclusively."

### 5. Additional Prompts
1. **Title and Summary Extraction Prompt:**
   ```
   You are an AI that extracts precise titles and summaries from documentation chunks.
   
   For the title:
   - If this is the start of a document, extract the exact title
   - If it's a section, create a title that includes the section hierarchy (e.g., "Authentication > OAuth2 > Refresh Tokens")
   - Create titles that are both descriptive and searchable with relevant technical terms
   - Keep titles under 80 characters
   
   For the summary:
   - Focus on the main technical concepts and functionality described
   - Include key entities, functions, methods, or APIs mentioned
   - Highlight any warnings, prerequisites, or important notes
   - Ensure the summary is self-contained and meaningful without additional context
   - Keep summaries between 100-150 characters
   
   Return a JSON object with 'title' and 'summary' keys.
   ```

2. **Chunking Decision Logic:**
   ```python
   def chunk_text(text: str, chunk_size: int = DEFAULT_CHUNK_SIZE) -> List[str]:
       """
       Split text into chunks respecting content boundaries with the following priorities:
       1. Never split in the middle of a code block
       2. Prefer to split between major sections (## headings)
       3. Split between paragraphs when possible
       4. Maintain table integrity - never split in the middle of a table
       5. Keep list items together when possible
       6. Ensure each chunk has sufficient context (title/header included)
       """
       # Implementation...
   ```

## Phase 5: Administration and Maintenance

### 1. Backup and Recovery
1. **Configure Regular Backups**
   - Set up `pg_dump` scheduled jobs for full database backups
   - Implement WAL archiving for point-in-time recovery
   - Test restore procedures to ensure data can be recovered

2. **Monitoring and Alerting**
   - Set up PostgreSQL monitoring (using tools like pgMonitor or Prometheus)
   - Configure alerts for disk space, connection limits, and query performance
   - Implement logging of slow queries for optimization

### 3. Performance Tuning
1. **PostgreSQL Configuration**
   - Tune `shared_buffers` based on available RAM (typically 25% of system memory)
   - Adjust `effective_cache_size` to reflect available memory
   - Configure `work_mem` for complex vector operations

2. **Index Maintenance**
   - Schedule regular VACUUM and ANALYZE operations
   - Monitor index usage and size
   - Rebuild fragmented indexes periodically

## Phase 6: Testing and Validation

### 1. Functional Testing
1. **Vector Search Validation**
   - Test vector search with sample queries
   - Verify that vector similarity scores are appropriate
   - Test with varying query complexity and result set sizes

2. **Performance Benchmarking**
   - Measure query latency under different loads
   - Test concurrent user scenarios
   - Optimize based on resource utilization metrics

3. **Pre-warming the Database**
   - Execute several thousand warm-up queries before production use
   - Monitor RAM utilization during warm-up
   - Adjust compute resources based on observed performance

### 2. Index Parameter Tuning
1. **HNSW Index Fine-tuning**
   - Experiment with different `m` values (connections per node)
   - Adjust `ef_construction` (accuracy during build time)
   - Optimize `ef_search` parameter (accuracy during search time)

2. **IVFFlat Index Fine-tuning (Alternative Approach)**
   - Tune the `lists` parameter (number of clusters)
   - Adjust `probes` parameter (clusters to search)
   - Balance between search speed and recall accuracy
